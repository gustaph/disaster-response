{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "from sqlalchemy import create_engine\r\n",
    "from pickle import dump\r\n",
    "\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
    "from sklearn.multioutput import MultiOutputClassifier\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from lightgbm import LGBMClassifier\r\n",
    "\r\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('stopwords')\r\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gusta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# load data from database\r\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db') # access data from database\r\n",
    "df = pd.read_sql_table('disaster_response', engine) # get data from database\r\n",
    "\r\n",
    "category_columns = df.drop(['id', 'message', 'original', 'genre'], axis=1).columns\r\n",
    "\r\n",
    "X = df['message'].values\r\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis=1).values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# regex obtained from Udacity course classes\r\n",
    "url_regex = \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\r\n",
    "\r\n",
    "def tokenize(text: str) -> list:\r\n",
    "    '''\r\n",
    "    function that transforms raw text into clean tokens\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        - text: str -> text to be tokenized\r\n",
    "    Returns:\r\n",
    "        - token list\r\n",
    "    '''\r\n",
    "    \r\n",
    "    clean_text = re.sub(r'[^a-z-A-Z]|\\W', ' ', text.lower()) # remove everything but characters\r\n",
    "    clean_text = re.sub(url_regex, 'urlplaceholder', clean_text) # replace url's by `urlplaceholder`\r\n",
    "    \r\n",
    "    tokens = word_tokenize(clean_text) # tokenize text\r\n",
    "    lemmatizer = WordNetLemmatizer() # instatiate lemmatizer\r\n",
    "    \r\n",
    "    clean_tokens = []\r\n",
    "    for token in tokens:\r\n",
    "        clean_token = lemmatizer.lemmatize(token).strip() # lemmatize word\r\n",
    "        clean_token = lemmatizer.lemmatize(clean_token, pos='v') # lemmatize again operating by verbs\r\n",
    "        \r\n",
    "        if clean_token not in stopwords.words(\"english\"): # remove stopwords\r\n",
    "            clean_tokens.append(clean_token)\r\n",
    "    \r\n",
    "    return clean_tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# instantiate the main Pipeline with MultiOutputClassifier based on RandomForestClassifer\r\n",
    "\r\n",
    "pipeline = Pipeline([\r\n",
    "    ('count', CountVectorizer(tokenizer=tokenize)), # bag of words based on `tokenize` function\r\n",
    "    ('tfidf', TfidfTransformer()), # TF-IDF transformer\r\n",
    "    ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1), n_jobs=-1)), \r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y) # split data into train & test sets\r\n",
    "pipeline.fit(X_train, y_train) # fit the data on the classifier"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000253809AA0D0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1),\n",
       "                                       n_jobs=-1))])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_classification(model, X_test: np.array, y_test: pd.DataFrame) -> None:\r\n",
    "    '''\r\n",
    "    shows the overall ranking of each target column\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        - model: classification model\r\n",
    "        - X_test: x values for test\r\n",
    "        - y_test: y values for test\r\n",
    "    Returns: all column classification report\r\n",
    "    '''\r\n",
    "    \r\n",
    "    y_pred = model.predict(X_test) # predict data\r\n",
    "    \r\n",
    "    for i, column in enumerate(category_columns):\r\n",
    "        print(f\"({column}):\")\r\n",
    "        print(classification_report(y_test[:,i], y_pred[:,i], zero_division=1))\r\n",
    "    \r\n",
    "    print(\"TOTAL ACC:\", (y_pred == y_test).mean())\r\n",
    "        \r\n",
    "    return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def test_model(model, text='rain followed by hurricane left thousands of people starving and thirsty') -> list:\r\n",
    "    '''\r\n",
    "    function used to test model prediction\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        - model: trained model\r\n",
    "        - text: disaster news to be predicted\r\n",
    "    Returns: disaster response according to model prediction\r\n",
    "    '''\r\n",
    "    classification_labels = model.predict([text])[0]\r\n",
    "    classification_results = dict(zip(df.columns[4:], classification_labels))\r\n",
    "    \r\n",
    "    return [key for key, value in classification_results.items() if value == 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "test_model(pipeline)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['related', 'aid_related', 'water', 'food', 'weather_related', 'storm']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# report of all rows\r\n",
    "get_classification(pipeline, X_test, y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "pipeline.get_params() # get the classifier parameters"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('count',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x0000023D809DC550>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1), n_jobs=-1))],\n",
       " 'verbose': False,\n",
       " 'count': CountVectorizer(tokenizer=<function tokenize at 0x0000023D809DC550>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(n_jobs=-1), n_jobs=-1),\n",
       " 'count__analyzer': 'word',\n",
       " 'count__binary': False,\n",
       " 'count__decode_error': 'strict',\n",
       " 'count__dtype': numpy.int64,\n",
       " 'count__encoding': 'utf-8',\n",
       " 'count__input': 'content',\n",
       " 'count__lowercase': True,\n",
       " 'count__max_df': 1.0,\n",
       " 'count__max_features': None,\n",
       " 'count__min_df': 1,\n",
       " 'count__ngram_range': (1, 1),\n",
       " 'count__preprocessor': None,\n",
       " 'count__stop_words': None,\n",
       " 'count__strip_accents': None,\n",
       " 'count__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'count__tokenizer': <function __main__.tokenize(text: str) -> list>,\n",
       " 'count__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': -1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(n_jobs=-1),\n",
       " 'clf__n_jobs': -1}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "> REFERENCES:\r\n",
    "    * Hyperparameter Tuning the Random Forest in Python [https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74];\r\n",
    "    * Tuning the parameters of your Random Forest model [https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/] \r\n",
    "'''\r\n",
    "\r\n",
    "parameters = {'clf__estimator__n_estimators' : (10, 20, 50),   \r\n",
    "              'clf__estimator__max_features': ('auto', 'sqrt'),\r\n",
    "              'clf__estimator__max_depth': (None, 10, 50, 100),\r\n",
    "              'clf__estimator__min_samples_leaf': (1, 10, 50),\r\n",
    "            }\r\n",
    "\r\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters) # operating GridSearch to find best parameters\r\n",
    "cv.fit(X_train, y_train) # fit model on split data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(\"BEST ESTIMATOR:\")\r\n",
    "print(cv.best_estimator_)  # best estimator found by GridSearch\r\n",
    "print(\"BEST PARAMS:\")\r\n",
    "print(cv.best_params_) # best parameters of the estimator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# result\r\n",
    "\r\n",
    "pipeline = Pipeline([\r\n",
    "    ('count', CountVectorizer(tokenizer=tokenize)), # bag of words based on `tokenize` function\r\n",
    "    ('tfidf', TfidfTransformer()), # TF-IDF transformer\r\n",
    "    ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(\r\n",
    "        n_estimators=20, max_features='sqrt', n_jobs=-1), n_jobs=-1)), \r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pipeline.fit(X_train, y_train) # fit the data on the classifier"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000253809AA0D0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(max_features='sqrt',\n",
       "                                                                        n_estimators=20,\n",
       "                                                                        n_jobs=-1),\n",
       "                                       n_jobs=-1))])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# report of all rows\n",
    "get_classification(pipeline, X_test, y_test)\n",
    "test_model(pipeline)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['related',\n",
       " 'aid_related',\n",
       " 'water',\n",
       " 'food',\n",
       " 'weather_related',\n",
       " 'storm',\n",
       " 'direct_report']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **LGBMClassifier**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "'''\n",
    "> REFERENCES:\n",
    "    * Understanding LightGBM Parameters (and How to Tune Them) [https://neptune.ai/blog/lightgbm-parameters-guide]\n",
    "'''\n",
    "\n",
    "PARAMS = {'learning_rate': 0.2,\n",
    "          'max_depth': 10,\n",
    "          'num_leaves': 20,\n",
    "          'feature_fraction': 0.6,\n",
    "          'subsample': 0.2,\n",
    "          'class_weight': 'balanced',\n",
    "          'n_estimators': 30\n",
    "         }\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('count', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(estimator=LGBMClassifier(**PARAMS), n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline2.fit(X_train, y_train) # fit the data on the classifier"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000253809AA0D0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=LGBMClassifier(class_weight='balanced',\n",
       "                                                                feature_fraction=0.6,\n",
       "                                                                learning_rate=0.2,\n",
       "                                                                max_depth=10,\n",
       "                                                                n_estimators=30,\n",
       "                                                                num_leaves=20,\n",
       "                                                                subsample=0.2),\n",
       "                                       n_jobs=-1))])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "test_model(pipeline2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['related',\n",
       " 'aid_related',\n",
       " 'food',\n",
       " 'death',\n",
       " 'weather_related',\n",
       " 'storm',\n",
       " 'other_weather',\n",
       " 'direct_report']"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "print(test_model(pipeline, 'lightning destroys building leaving many injured'))\n",
    "print(test_model(pipeline2, 'lightning destroys building leaving many injured'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['related', 'aid_related', 'medical_help']\n",
      "['related', 'medical_help', 'infrastructure_related', 'buildings', 'other_infrastructure']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **MultinomialNB**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "pipeline3 = Pipeline([\n",
    "    ('count', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(estimator=MultinomialNB(), n_jobs=-1)),\n",
    "])\n",
    "\n",
    "pipeline3.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x0000023D809DC550>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=MultinomialNB(), n_jobs=-1))])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "test_model(pipeline3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['related', 'aid_related', 'weather_related']"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "parameters = {\n",
    "        'count__max_df': (0.5, 0.75, 1.0),\n",
    "        'clf__estimator__alpha': (0.5, 0.75, 1.0),\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline3, parameters, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('count',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x0000023D809DC550>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=MultinomialNB(),\n",
       "                                                              n_jobs=-1))]),\n",
       "             param_grid={'clf__estimator__alpha': (0.5, 0.75, 1.0),\n",
       "                         'count__max_df': (0.5, 0.75, 1.0)},\n",
       "             verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "test_model(grid_search)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['related', 'aid_related', 'weather_related', 'direct_report']"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "grid_search.best_estimator_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count',\n",
       "                 CountVectorizer(max_df=0.5,\n",
       "                                 tokenizer=<function tokenize at 0x0000023D809DC550>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=MultinomialNB(alpha=0.5),\n",
       "                                       n_jobs=-1))])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "after all, the **LGBMClassifier** model seems to be better compared to the others"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9. Export your model as a pickle file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# exporting the trained LGBMClassifier model as a pickle file\n",
    "with open('classifier.pkl', 'wb') as f:\n",
    "    dump(pipeline2, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}